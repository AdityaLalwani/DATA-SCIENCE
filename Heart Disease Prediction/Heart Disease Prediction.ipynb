{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_PRACTICAL2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBmeJXFF2sJu"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6v1ZfNOkrKQ"
      },
      "source": [
        "## Dataset: Heart Disease Prediction\n",
        "The dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on\n",
        "residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient\n",
        "has 10-year risk of future coronary heart disease (CHD).The dataset provides the patientsâ€™ information. It\n",
        "includes over 4,000 records and 15 attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15U7JIbXk9zn"
      },
      "source": [
        "### Importing Dataset with Pandas:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf_Nb8BOlCHi"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "4ux0IypZlI4y",
        "outputId": "b60b8005-e816-451e-a034-30dc028863c1"
      },
      "source": [
        "df = pd.read_csv(r'/content/sample_data/framingham.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>male</th>\n",
              "      <th>age</th>\n",
              "      <th>education</th>\n",
              "      <th>currentSmoker</th>\n",
              "      <th>cigsPerDay</th>\n",
              "      <th>BPMeds</th>\n",
              "      <th>prevalentStroke</th>\n",
              "      <th>prevalentHyp</th>\n",
              "      <th>diabetes</th>\n",
              "      <th>totChol</th>\n",
              "      <th>sysBP</th>\n",
              "      <th>diaBP</th>\n",
              "      <th>BMI</th>\n",
              "      <th>heartRate</th>\n",
              "      <th>glucose</th>\n",
              "      <th>TenYearCHD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>26.97</td>\n",
              "      <td>80.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>250.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>28.73</td>\n",
              "      <td>95.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>48</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>245.0</td>\n",
              "      <td>127.5</td>\n",
              "      <td>80.0</td>\n",
              "      <td>25.34</td>\n",
              "      <td>75.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>28.58</td>\n",
              "      <td>65.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>285.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>23.10</td>\n",
              "      <td>85.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   male  age  education  currentSmoker  ...    BMI  heartRate  glucose  TenYearCHD\n",
              "0     1   39        4.0              0  ...  26.97       80.0     77.0           0\n",
              "1     0   46        2.0              0  ...  28.73       95.0     76.0           0\n",
              "2     1   48        1.0              1  ...  25.34       75.0     70.0           0\n",
              "3     0   61        3.0              1  ...  28.58       65.0    103.0           1\n",
              "4     0   46        3.0              1  ...  23.10       85.0     85.0           0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkrIPwEymRs4"
      },
      "source": [
        "X = df.iloc[:, :-1] # X has all columns except the last column because it is the column we have to make predictions for.\n",
        "y = df.iloc[:, -1] # y has last column\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nWKGwAuAKQx"
      },
      "source": [
        "## Standardizing Data\n",
        "\n",
        "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
        "\n",
        "With standardizing, we can take attributes with a Gaussian distribution and different means and standard deviations and transform them into a standard Gaussian distribution with a mean of 0 and a standard deviation of 1. For this, we use the StandardScaler class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAuLNfkMAIdm"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUdm6ZErARDV",
        "outputId": "3ba13325-d9fc-4456-f68e-9fbe9cd296aa"
      },
      "source": [
        "scaler=StandardScaler().fit(X)\n",
        "rescaledX=scaler.transform(X)\n",
        "rescaledX"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.1531919 , -1.23495068,  1.98206814, ...,  0.28629879,\n",
              "         0.342704  , -0.20732048],\n",
              "       [-0.86715836, -0.41825733,  0.02064407, ...,  0.71771073,\n",
              "         1.59008688, -0.24906213],\n",
              "       [ 1.1531919 , -0.18491638, -0.96006796, ..., -0.11324749,\n",
              "        -0.0730903 , -0.49951204],\n",
              "       ...,\n",
              "       [-0.86715836, -0.18491638,  0.02064407, ..., -0.93194969,\n",
              "         0.67533943,  0.16835438],\n",
              "       [-0.86715836, -0.65159829, -0.96006796, ..., -1.62809168,\n",
              "         0.84165715,         nan],\n",
              "       [-0.86715836,  0.28176554,  0.02064407, ..., -1.06186351,\n",
              "         0.342704  ,  1.04492906]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPA_L9GWm-55",
        "outputId": "cb0893b1-a2c2-43fc-fb65-0b5e129f13d6"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "x = MinMaxScaler(feature_range=(0,5))\n",
        "scale = x.fit_transform(X)\n",
        "scale"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.        , 0.92105263, 5.        , ..., 1.38511876, 1.81818182,\n",
              "        0.52259887],\n",
              "       [0.        , 1.84210526, 1.66666667, ..., 1.59840039, 2.57575758,\n",
              "        0.50847458],\n",
              "       [5.        , 2.10526316, 0.        , ..., 1.18759089, 1.56565657,\n",
              "        0.42372881],\n",
              "       ...,\n",
              "       [0.        , 2.10526316, 1.66666667, ..., 0.78284052, 2.02020202,\n",
              "        0.64971751],\n",
              "       [0.        , 1.57894737, 0.        , ..., 0.43868153, 2.12121212,\n",
              "               nan],\n",
              "       [0.        , 2.63157895, 1.66666667, ..., 0.71861367, 1.81818182,\n",
              "        0.94632768]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB_x-m1knskb"
      },
      "source": [
        "## Imputation of missing values\n",
        "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning.\n",
        "\n",
        "A better strategy is to impute the missing values, i.e., to infer them from the known part of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoMBOBMdq-Gh"
      },
      "source": [
        "from sklearn.impute import SimpleImputer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pDLHC_MoP_M",
        "outputId": "c1188d9e-0646-4a17-ad03-117c93ecf7a9"
      },
      "source": [
        "mean_values = SimpleImputer(strategy='mean')\n",
        "data1 = mean_values.fit_transform(rescaledX)\n",
        "print(data1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.15319190e+00 -1.23495068e+00  1.98206814e+00 ...  2.86298790e-01\n",
            "   3.42703997e-01 -2.07320483e-01]\n",
            " [-8.67158360e-01 -4.18257334e-01  2.06440713e-02 ...  7.17710727e-01\n",
            "   1.59008688e+00 -2.49062135e-01]\n",
            " [ 1.15319190e+00 -1.84916377e-01 -9.60067961e-01 ... -1.13247493e-01\n",
            "  -7.30902975e-02 -4.99512044e-01]\n",
            " ...\n",
            " [-8.67158360e-01 -1.84916377e-01  2.06440713e-02 ... -9.31949692e-01\n",
            "   6.75339433e-01  1.68354381e-01]\n",
            " [-8.67158360e-01 -6.51598291e-01 -9.60067961e-01 ... -1.62809168e+00\n",
            "   8.41657150e-01  1.75328727e-16]\n",
            " [-8.67158360e-01  2.81765538e-01  2.06440713e-02 ... -1.06186351e+00\n",
            "   3.42703997e-01  1.04492906e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojvzJq8rBLFz"
      },
      "source": [
        "## Normalizing Data\n",
        "\n",
        "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
        "\n",
        "In this task, we rescale each observation to a length of 1 (a unit norm). For this, we use the Normalizer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t_nRd7mBPhm"
      },
      "source": [
        "from sklearn import preprocessing"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSOfQN-_BR3F",
        "outputId": "878e132e-cbc1-45cd-d581-fd552385efcf"
      },
      "source": [
        "normaliseX = preprocessing.normalize(data1)\n",
        "normaliseX"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.25334840e-01, -3.48400368e-01,  5.59174772e-01, ...,\n",
              "         8.07697060e-02,  9.66825640e-02, -5.84885968e-02],\n",
              "       [-3.40842556e-01, -1.64398921e-01,  8.11429419e-03, ...,\n",
              "         2.82101137e-01,  6.24994582e-01, -9.78955847e-02],\n",
              "       [ 5.11451031e-01, -8.20120842e-02, -4.25798817e-01, ...,\n",
              "        -5.02262864e-02, -3.24162075e-02, -2.21538106e-01],\n",
              "       ...,\n",
              "       [-3.73101622e-01, -7.95617079e-02,  8.88227210e-03, ...,\n",
              "        -4.00978596e-01,  2.90570038e-01,  7.24357804e-02],\n",
              "       [-3.10566762e-01, -2.33365416e-01, -3.43841693e-01, ...,\n",
              "        -5.83089763e-01,  3.01433681e-01,  6.27927696e-17],\n",
              "       [-3.61438172e-01,  1.17442010e-01,  8.60460528e-03, ...,\n",
              "        -4.42592754e-01,  1.42841621e-01,  4.35534347e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSpzxTyqsbQv"
      },
      "source": [
        "## Discretization\n",
        "Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes.\n",
        "\n",
        "One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWMAhaF9uMxS"
      },
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr-tLtZruHE2",
        "outputId": "ac65467a-9899-441a-d82c-a9df330e449b"
      },
      "source": [
        "dis = KBinsDiscretizer(n_bins=10, encode='ordinal')\n",
        "dis.fit(normaliseX)\n",
        "print(dis.n_bins)\n",
        "print(dis.bin_edges_[0])\n",
        "print(dis.bin_edges_[4])\n",
        "print(dis.bin_edges_[9])\n",
        "preProcessedX = dis.fit_transform(normaliseX)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "[-0.49669172 -0.34080554 -0.30418808 -0.27441178 -0.24593426 -0.20002433\n",
            "  0.22525374  0.31776493  0.3587834   0.40845888  0.5971696 ]\n",
            "[-4.14119853e-01 -2.86338026e-01 -2.55805549e-01 -2.29783080e-01\n",
            " -2.04283776e-01 -1.41188374e-01 -9.30753395e-05  1.66883246e-01\n",
            "  2.92253007e-01  4.06268438e-01  9.24550479e-01]\n",
            "[-0.78006557 -0.3712633  -0.26001283 -0.1696345  -0.089706   -0.01625665\n",
            "  0.04543237  0.12787188  0.23103746  0.37457507  0.96144485]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htz7f_WOwz66",
        "outputId": "69fc7ea4-f2a7-42a4-ef6d-50ca54f6ecc3"
      },
      "source": [
        "preProcessedX"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7., 1., 9., ..., 6., 6., 4.],\n",
              "       [0., 3., 6., ..., 8., 9., 3.],\n",
              "       [9., 4., 0., ..., 4., 4., 0.],\n",
              "       ...,\n",
              "       [0., 4., 6., ..., 0., 8., 7.],\n",
              "       [1., 2., 1., ..., 0., 8., 6.],\n",
              "       [0., 6., 6., ..., 0., 7., 9.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P01fuipHaKR"
      },
      "source": [
        "## Encoding\n",
        "\n",
        "When dealing with few and scattered numerical values, we may not need to store these. Then, we can perform One Hot Encoding. For k distinct values, we can transform the feature into a k-dimensional vector with one value of 1 and 0 as the rest values. to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKTABj1dsAEe"
      },
      "source": [
        "### OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp2S9jTBMIvg"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDA6QgwMMLVg",
        "outputId": "1c0ab89d-a308-4645-ecdc-d5577fd59ceb"
      },
      "source": [
        "encoder=OneHotEncoder()\n",
        "s = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
        "encoder.fit(s)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
              "              handle_unknown='error', sparse=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vS7asVKMcJv",
        "outputId": "6a6ebcdd-af5c-43f5-8891-bb594e00a799"
      },
      "source": [
        "encoder.transform([['female', 'from US', 'uses Safari'],['male', 'from Europe', 'uses Safari']]).toarray()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 1., 0., 1.],\n",
              "       [0., 1., 1., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwhmeaBgsIdX"
      },
      "source": [
        "### OrdinalEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uda3pPe9sNGR",
        "outputId": "cbb6707e-aa97-4c2b-cbfe-5ec0235b6bff"
      },
      "source": [
        "enc = preprocessing.OrdinalEncoder()\n",
        "s = [['a','b','c','d'], ['e', 'f', 'g', 'h'],['i','j','k','l']]\n",
        "enc.fit(s)\n",
        "enc.transform([['a', 'f', 'g','l']])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 2.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}